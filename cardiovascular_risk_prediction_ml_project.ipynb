{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hussainaarish7/cardiovascular-risk-prediction-uisng-ml-model/blob/main/cardiovascular_risk_prediction_ml_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "#### **Contribution**    - Individual\n",
        "##### **Made By**       - Mohd Aarish   \n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "emDvHEKc44G-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In pursuit of forecasting the 10-year likelihood of coronary heart disease (CHD) in patients, machine learning techniques were harnessed, leveraging data from an ongoing cardiovascular study centered on Framingham, Massachusetts residents. This dataset encapsulated insights on over 4,000 patients, spanning 15 attributes representing potential CHD risk factors, ranging from demographic to medical markers.\n",
        "\n",
        "Data preparation entailed meticulous preprocessing, involving robust techniques to cleanse and morph the dataset. Missing values were tactfully addressed via median, mode, and KNN imputation methodologies. Furthermore, outliers were meticulously detected and expunged leveraging the Interquartile Range (IQR) strategy. Additionally, skewed continuous variables underwent transformative treatments like logarithmic and square root transformations to mitigate skewness, thereby enhancing model efficacy.\n",
        "\n",
        "Feature selection was orchestrated, leveraging the variance inflation factor to eradicate multicollinearity. Notably, a novel attribute termed \"pulse pressure\" was engineered to encapsulate the interplay between systolic and diastolic blood pressure. Superfluous columns were pruned to streamline the dataset. Key predictors for CHD risk were pinpointed, encompassing 'age', 'sex', 'education', 'cigs_per_day', 'bp_meds', 'prevalent_stroke', 'prevalent_hyp', 'diabetes', 'total_cholesterol', 'bmi', 'heart_rate', 'glucose', and 'pulse_pressure'.\n",
        "\n",
        "To tackle the dataset's inherent imbalance, the SMOTE coupled with Tomek links undersampling methodology was enlisted to harmonize class distribution, thereby amplifying model performance. Additionally, data scaling via the standard scalar method was executed to ensure feature uniformity.\n",
        "\n",
        "A myriad of machine learning models underwent scrutiny, with emphasis on their recall performance. Following rigorous assessment, the tuned Neural Network emerged triumphant as the ultimate predictive model, boasting the highest recall score amidst its contenders. Prioritizing a model with elevated recall aimed to adeptly discern patients at CHD risk, even at the cost of potential false positives.\n",
        "\n",
        "This endeavor underscored the potency of machine learning methodologies in accurately prognosticating CHD risk in patients, leveraging insights gleaned from cardiovascular studies. By navigating through meticulous preprocessing, judicious feature selection, and model curation grounded on pertinent evaluation metrics, the project heralded a positive stride towards precise CHD risk prediction, fostering tangible business impact."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/hussainaarish7/cardiovascular-risk-prediction-uisng-ml-model"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "coronary heart disease (CHD) remains a leading cause of morbidity and mortality worldwide. Early identification of individuals at high risk of developing CHD is crucial for implementing preventive interventions and reducing disease burden. The Framingham Heart Study provides valuable longitudinal data on cardiovascular risk factors, offering an opportunity to develop predictive models for estimating the 10-year risk of CHD in residents of Framingham, Massachusetts."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime as dt\n",
        "\n",
        "\n",
        "# Import Visualization Libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, classification_report\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTETomek\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# Import warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "url = 'https://raw.githubusercontent.com/hussainaarish7/cardiovascular-risk-prediction-uisng-ml-model/main/data_cardiovascular_risk.csv'\n",
        "data = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First look\n",
        "data.head(10)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail()"
      ],
      "metadata": {
        "id": "b3JFxXyZ289C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the id column (not relevant)\n",
        "data.drop(columns=['id'], inplace=True)\n"
      ],
      "metadata": {
        "id": "QA5hdkrZfVmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "data.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "data.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "data.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing value\n",
        "# Create heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(data.isnull(), cmap='viridis', cbar=False)\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.xlabel('Variables')\n",
        "plt.ylabel('Observations')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "our dataset contains 3390 rows and 17 columns.it has no dublicate value but in our dataset contains some missing values"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "data.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "data.describe(include='all').round(2)"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Demographic           |                                                              |\n",
        "|-----------------------|--------------------------------------------------------------|\n",
        "| Sex                   | Male (M) or Female (F)                                      |\n",
        "| Age                   | Age of the patient (Whole numbers, but conceptually continuous) |\n",
        "| Education             | Level of education: 1, 2, 3, or 4                            |\n",
        "\n",
        "| Behavioral            |                                                              |\n",
        "|-----------------------|--------------------------------------------------------------|\n",
        "| is_smoking            | Current smoker (YES) or Non-smoker (NO)                      |\n",
        "| Cigs Per Day          | Average number of cigarettes smoked per day (Continuous)      |\n",
        "\n",
        "| Medical (History)     |                                                              |\n",
        "|-----------------------|--------------------------------------------------------------|\n",
        "| BP Meds               | On blood pressure medication (Yes or No)                     |\n",
        "| Prevalent Stroke      | History of stroke (Yes or No)                                |\n",
        "| Prevalent Hyp         | Hypertensive (Yes or No)                                     |\n",
        "| Diabetes              | Diabetic (Yes or No)                                         |\n",
        "\n",
        "| Medical (Current)     |                                                              |\n",
        "|-----------------------|--------------------------------------------------------------|\n",
        "| Tot Chol              | Total cholesterol level (Continuous)                         |\n",
        "| Sys BP                | Systolic blood pressure (Continuous)                         |\n",
        "| Dia BP                | Diastolic blood pressure (Continuous)                        |\n",
        "| BMI                   | Body Mass Index (Continuous)                                 |\n",
        "| Heart Rate            | Heart rate (Continuous)                                      |\n",
        "| Glucose               | Glucose level (Continuous)                                   |\n",
        "\n",
        "| Predict Variable      |                                                              |\n",
        "|-----------------------|--------------------------------------------------------------|\n",
        "| 10-year risk of CHD   | Risk of coronary heart disease in 10 years (Binary: 1 = Yes, 0 = No) |"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "# List of variables\n",
        "variables = ['age', 'education', 'sex', 'is_smoking', 'cigsPerDay', 'BPMeds',\n",
        "             'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol', 'sysBP',\n",
        "             'diaBP', 'BMI', 'heartRate', 'glucose', 'TenYearCHD']\n",
        "\n",
        "# Check unique values for each variable\n",
        "for variable in variables:\n",
        "    unique_values = data[variable].nunique()\n",
        "    print(f\"Unique values for '{variable}':\")\n",
        "    print(unique_values)\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Before doing any data wrangling lets create copy of the dataset\n",
        "df = data.copy()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming the columns\n",
        "df.rename(columns={'cigsPerDay':'cigs_per_day','BPMeds':'bp_meds',\n",
        "                   'prevalentStroke':'prevalent_stroke','prevalentHyp':'prevalent_hyp',\n",
        "                   'totChol':'total_cholesterol','sysBP':'systolic_bp','diaBP':'diastolic_bp',\n",
        "                   'BMI':'bmi','heartRate':'heart_rate','TenYearCHD':'ten_year_chd'},\n",
        "          inplace = True)"
      ],
      "metadata": {
        "id": "FVJ53ZnQAJBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining 3 lists containing the column names of\n",
        "# This is defined based on the number of unique values for each attribute\n",
        "dependent_var = ['ten_year_chd']\n",
        "\n",
        "categorical_var = [i for i in df.columns if df[i].nunique()<=4]\n",
        "continuous_var = [i for i in df.columns if i not in categorical_var]"
      ],
      "metadata": {
        "id": "VBWs9LNYAPU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_var"
      ],
      "metadata": {
        "id": "L-c-yhgdAwnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_var"
      ],
      "metadata": {
        "id": "tXmwSvJZBEfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changed the names of all the columns for ease of use.\n",
        "\n",
        "We have also defined the continuous variables, dependent variable and categorical variables for ease of plotting graphs."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "# Distribution of dependent varaible - ten_year_chd\n",
        "\n",
        "\n",
        "# Dependant Column Value Counts\n",
        "print(df.ten_year_chd.value_counts())\n",
        "print(\" \")\n",
        "\n",
        "#color palette selection\n",
        "colors = sns.color_palette(\"Paired\")\n",
        "\n",
        "# plotting data on chart\n",
        "plt.figure(figsize=(12,7))\n",
        "explode = [0,0.1]\n",
        "textprops = {'fontsize':13}\n",
        "plt.pie(df['ten_year_chd'].value_counts(), labels=['Not CHD(%)','CHD(%)'], startangle=90, colors=colors, explode = explode, autopct=\"%1.1f%%\",textprops = textprops)\n",
        "plt.title('Ten Year CHD (%)', fontsize=20)\n",
        "\n",
        "# displaying chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "here i used pie chart because pie chart is a good visualization method to view the proportions of all the values of our particular variable"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart we come to know that 15.1% that is 511 out of 3390 are classified as positive for 10 year CHD whereas the remaining 84.9% that is 2879 out of 3390 are classified as negative for 10 year CHD."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights from the chart show that 15.1% of the population has a positive classification for 10-year CHD, while 84.9% have a negative classification. This information can help businesses in the healthcare industry develop targeted strategies. There are no specific insights in the chart that indicate negative growth, but failure to address high CHD prevalence could have negative implications for public health and healthcare businesses."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Visualizing code of histogram plot & boxplot for each columns to know the data distribution\n",
        "for col in df.describe().columns:\n",
        "    fig,axes = plt.subplots(nrows=1,ncols=2,figsize=(18,6))\n",
        "    sns.histplot(df[col], ax = axes[0],kde = True)\n",
        "    sns.boxplot(df[col], ax = axes[1],orient='h',showmeans=True,color='pink')\n",
        "    fig.suptitle(\"Distribution plot of \"+ col, fontsize = 15)\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histplot is a type of chart that displays the distribution of a dataset. It is a graphical representation of the data that shows how often each value or group of values occurs. Histplots are useful for understanding the distribution of a dataset and identifying patterns or trends in the data. It is also useful when dealing with large data sets (greater than 100 observations). It can help detect any unusual observations (outliers) or any gaps in the data.\n",
        "\n",
        "Thus, I used the histogram plot to analysis the variable distributions over the whole dataset whether it's symmetric or not.\n",
        "\n",
        "A boxplot is used to summarize the key statistical characteristics of a dataset, including the median, quartiles, and range, in a single plot. Boxplots are useful for identifying the presence of outliers in a dataset, comparing the distribution of multiple datasets, and understanding the dispersion of the data. They are often used in statistical analysis and data visualization.\n",
        "\n",
        "Thus, for each numerical varibale in the given dataset, I used box plot to analyse the outliers and interquartile range including mean, median, maximum and minimum value."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the distribution we see that most of the columns are normally distributed, some of them are skewed and we can see some of the categorical columns also. In the box plot, we see some outliers also which we will handle later before model building."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Histogram and Box plot cannot give us whole information regarding data. It's done just to see the distribution of the column data over the dataset and the outliers in the boxplot for the different continuous columns."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "# Analysing the distribution of categorical variables in the dataset\n",
        "for i in categorical_var:\n",
        "  plt.figure(figsize=(10,5))\n",
        "  p = sns.countplot(x=i, data = df)\n",
        "  plt.xlabel(i)\n",
        "  plt.title(i+' distribution')\n",
        "  for i in p.patches:\n",
        "    p.annotate(f'{i.get_height()}', (i.get_x() + i.get_width() / 2., i.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of different categories in the categorical columns can be seen. The education column has the highest for the 1 category followed by 2 3 and 4.\n",
        "\n",
        "The gender distribution is not even with high count for females. The is_smoking column is even. Bp_meds, prevalent_stroke, prevalent_hyp and diabetes are imbalanced, they have very few counts for the positive cases.\n",
        "\n",
        "Finaly the ten_year_chd is also imbalanced with few positive cases compared to the negative cases."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the chart can potentially create a positive business impact by providing valuable information for decision-making. Understanding the distribution of categories in various columns helps identify patterns and target specific demographics or areas of focus. For example, businesses can develop tailored marketing campaigns based on the gender distribution or design educational programs based on the education levels of the target audience. Additionally, recognizing the imbalanced distribution of health conditions can guide businesses in developing specialized treatments or preventive measures to address specific needs.\n",
        "\n",
        "However, it is important to note that the chart alone does not provide a complete picture of the business impact. The actual impact would depend on how well these insights are utilized in business strategies and actions. Factors such as market demand, competition, and the effectiveness of the implemented strategies would also play a role in determining the overall business growth. Therefore, while the gained insights have the potential to create positive business impact, their actual realization would require further analysis and strategic implementation."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "# Relationship between the dependent variable and continuous independent variables\n",
        "for i in continuous_var:\n",
        "  plt.figure(figsize=(10,5))\n",
        "  sns.catplot(x=dependent_var[0],y=i,data=df,kind='violin')\n",
        "  plt.ylabel(i)\n",
        "  plt.xlabel(dependent_var[0])\n",
        "  plt.title(dependent_var[0]+' vs '+i)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The violin chart is a type of data visualization that combines aspects of a box plot and a kernel density plot. It is used to display the distribution and density of data across different categories or groups. Some features of the violin chart include:\n",
        "\n",
        "Shape and width: The shape of the violin represents the data distribution, typically displaying a mirrored, symmetrical shape. The width of the violin at different points indicates the density of data.\n",
        "\n",
        "Quartiles and median: The central \"box\" in the violin chart represents the interquartile range (IQR) and contains the median value. This provides insights into the spread and central tendency of the data.\n",
        "\n",
        "Grouping and comparison: Violin charts can be grouped or arranged side by side to compare distributions across different categories or groups. This allows for visual comparisons of data distribution shapes, spreads, and densities."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For age vs ten_year_chd, we see that the density for positive cases is high at higher age as compared to lower age indicating that the positive cases are higher in older people.\n",
        "\n",
        "For cigs_per_day, the negative cases are more for the non smokers compared to the positive cases for non smokers.\n",
        "\n",
        "For ten_year_chd and glucose, the negative cases have high density compared to the positive cases for the same value of glucose.\n",
        "\n",
        "The remaining charts do not provide much information."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the violin chart can potentially create a positive business impact by providing valuable information for decision-making. Understanding the distribution and density of data across different categories can help businesses identify patterns, trends, and potential areas of focus. For example, the insight that positive cases of ten-year CHD are higher in older people suggests the need for targeted preventive measures or specialized treatments for this demographic. Similarly, the insight regarding the relationship between smoking and negative cases of CHD can inform smoking cessation programs or campaigns to reduce the risk of CHD.\n",
        "\n",
        "While the insights gained from the chart can be valuable, it's important to note that the impact on business growth would depend on various factors. The actual business impact would require further analysis and strategic implementation of these insights. Additionally, without specific business context and objectives, it is challenging to determine if there are any insights that would directly lead to negative growth. However, using the insights to better understand the distribution of health conditions and risk factors can potentially help businesses in the healthcare industry develop more effective strategies and interventions to improve patient outcomes and drive positive growth."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "# 100% stacked bar chart\n",
        "\n",
        "for i in categorical_var[:-1]:\n",
        "    x_var, y_var = i, dependent_var[0]\n",
        "    plt.figure(figsize=(10,5))\n",
        "    df_grouped = df.groupby(x_var)[y_var].value_counts(normalize=True).unstack(y_var)*100\n",
        "    df_grouped.plot.barh(stacked=True)\n",
        "    plt.legend(\n",
        "        bbox_to_anchor=(1.05, 1),\n",
        "        loc=\"upper left\",\n",
        "        title=y_var)\n",
        "\n",
        "    plt.title(\"% of patients at the risk of CHD by: \"+i)\n",
        "    for ix, row in df_grouped.reset_index(drop=True).iterrows():\n",
        "        # print(ix, row)\n",
        "        cumulative = 0\n",
        "        for element in row:\n",
        "            if element > 0.1:\n",
        "                plt.text(\n",
        "                    cumulative + element / 2,\n",
        "                    ix,\n",
        "                    f\"{int(element)} %\",\n",
        "                    va=\"center\",\n",
        "                    ha=\"center\",\n",
        "                )\n",
        "            cumulative += element\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A 100% horizontal stacked bar chart is a type of data visualization that represents the composition or proportion of multiple categories within a whole. Some features of the stacked horizontal bar chart include:\n",
        "\n",
        "Comparison of proportions: The chart displays multiple categories or groups stacked horizontally, allowing for easy visual comparison of their relative proportions within the total. Each bar represents the whole, and the segments of the bar represent the different categories or components.\n",
        "\n",
        "Percentage representation: The stacked horizontal bar chart represents the categories as percentages of the whole. This allows for a clear understanding of the relative contributions of each category to the total.\n",
        "\n",
        "Distribution across categories: The chart provides insights into the distribution and composition of the data across different categories. It helps identify which categories contribute more or less to the overall composition.\n",
        "\n",
        "Facilitating trend analysis: By comparing stacked horizontal bar charts over different time periods or groups, it is possible to observe trends and changes in the composition of categories. This can provide insights into shifts in proportions or the relative importance of different categories over time."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The percentage of CHD as per education does not provide much information as it is similar throughout the different education categories.\n",
        "\n",
        "For sex we see that there is a slightly higher chance of CHD in male than in female.\n",
        "\n",
        "For is_smoking again a very slightly high percentage of CHD is seen for the positive category of is_smoking.\n",
        "\n",
        "For bp_meds and diabetes, we see high percentage of CHD for positive cases compared to the negative case.\n",
        "\n",
        "Finally for the positive prevalent_stroke, the percentage is almost half indicating that the positive CHD is high for positive prevalent_stroke."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals the distribution and composition of different categories in relation to the occurrence of CHD. These insights can guide businesses in developing targeted strategies and interventions to address specific demographic groups or risk factors. For example, businesses can design preventive measures, educational campaigns, or tailored treatments to reduce the occurrence of CHD in high-risk categories. By leveraging these insights, businesses in the healthcare industry can improve patient outcomes, enhance customer satisfaction, and drive positive growth.\n",
        "\n",
        "There are no specific insights from the chart that directly lead to negative growth. However, it's important to consider the overall prevalence of CHD and the effectiveness of interventions. If the prevalence of CHD remains high across all categories and the implemented strategies fail to yield desired outcomes, it could potentially result in negative growth due to increased healthcare costs, decreased patient satisfaction, or reputational issues. Therefore, the business impact ultimately depends on the successful implementation of strategies based on the insights gained from the chart."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "k0O_TR13Lp9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# setting sns grid\n",
        "sns.set()\n",
        "\n",
        "# setting figure size\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# plotting catplot\n",
        "plot = sns.catplot(data=df, y='cigs_per_day', x='sex', hue='ten_year_chd', kind='bar')\n",
        "plot.set_xticklabels(['Female', 'Male'])\n",
        "plt.title(\"Consumption of cigarettes gender wise\")\n",
        "plt.ylabel(\"Cigarettes consumption per day\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "catplot is a powerful tool for visualizing the relationship between one categorical variable and one or more numerical or categorical variables. It's essentially a figure-level interface for drawing categorical plots onto a FacetGrid."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this visualization tell us that male are consuming more cigrets per day as compare to female population due to which males are at higher risk of coronary heart disease development under 10 years as comapare to females"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "this can help organisations to take some steps to reduce the consumptions of excess cigrets which is the cause of developing CHD"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# setting up the figure size\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# visualizing pointplot\n",
        "sns.pointplot(data=df, x='age', y='cigs_per_day', hue='ten_year_chd')\n",
        "plt.title(\"Risk of CHD due to cigarettes consumption in different age\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Cigarettes Consumption\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A point plot is a type of categorical plot in Seaborn that is used to visualize the relationship between two categorical variables and one numerical variable. It displays point estimates and confidence intervals as individual data points."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can conclude that young people consume more cigarettes as compared to old ones and hence they have a risk of coronary heart disease."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "yes this insights can help creating a positive business impact.It can help organisations or companies or health care profesionals to develop some campane which can educate the youths for the potential risk of taking cigrests or tobacco and its effect on our health"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# setting up the figure size\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# visualizing pointplot\n",
        "sns.pointplot(data=df, x='age', y='heart_rate', hue='ten_year_chd')\n",
        "plt.title(\"Risk of CHD due to heart rate in different age\")\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Heart Rate\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A point plot is a type of categorical plot in Seaborn that is used to visualize the relationship between two categorical variables and one numerical variable. It displays point estimates and confidence intervals as individual data points."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can conclude that old people have lower heart rate as compared to young ones and hence they have a risk of coronary heart disease."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "these insights can help govt. bodies and NGO to take some steps for eldery peoples to reduce the mortality rate by coronary heart disease"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "gs9S-DE3TIrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# setting up the figure size\n",
        "plt.figure(figsize=(15, 6))\n",
        "\n",
        "# visualizing scatterplot\n",
        "sns.scatterplot(data=df, x='bmi', y='total_cholesterol', hue='ten_year_chd')\n",
        "plt.title(\"Risk of CHD due to cholestrol and BMI\")\n",
        "plt.xlabel(\"Body Mass Index\")\n",
        "plt.ylabel(\"Cholestrol Level\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A scatter plot is a type of plot that displays the relationship between two numerical variables. Each data point in the plot represents the value of one variable against the value of another. Scatter plots are useful for visualizing patterns, trends, and relationships between variables."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a Healthy man :\n",
        "\n",
        "Cholestrol Level - less than 100\n",
        "Body mass index - between 18 to 25\n",
        "So we can conculde from the above scatter plot that an overweight person is overweighted due to high cholestrol level which is a major concern or risk to coronary heart disease."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "high cholestrol is a major consern amongs individuals it causes obasity which causes sereious health related problems which is alarming so this insights can help for the govt. and other organisations to take some steps and find the cause of high cholesterol amongs peoples to create awarness to overcome this serious problem\n"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# setting up the figure size\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# visualizing catplot\n",
        "sns.catplot(data=df, x='diabetes', y='glucose', kind='bar', hue='ten_year_chd')\n",
        "plt.title(\"Diabetes with glucose level\")\n",
        "plt.xlabel(\"Diabetes\")\n",
        "plt.ylabel(\"Glucose Level\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "catplot is a powerful tool for visualizing the relationship between one categorical variable and one or more numerical or categorical variables. It's essentially a figure-level interface for drawing categorical plots onto a FacetGrid."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we can conclude that if a person have high glucose level it means he/she is a patient of diabetes and is a risk factor for coronary heart disease.\n",
        "\n"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals the distribution and composition of different categories in relation to the occurrence of CHD. These insights can guide businesses in developing targeted strategies and interventions to address specific demographic groups or risk factors. For example, businesses can design preventive measures, educational campaigns, or tailored treatments to reduce the occurrence of CHD in high-risk categories. By leveraging these insights, businesses in the healthcare industry can improve patient outcomes, enhance customer satisfaction, and drive positive growth.\n",
        "\n",
        "There are no specific insights from the chart that directly lead to negative growth. However, it's important to consider the overall prevalence of CHD and the effectiveness of interventions. If the prevalence of CHD remains high across all categories and the implemented strategies fail to yield desired outcomes, it could potentially result in negative growth due to increased healthcare costs, decreased patient satisfaction, or reputational issues. Therefore, the business impact ultimately depends on the successful implementation of strategies based on the insights gained from the chart."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "LiTTsQ2pZzgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "fig, ax = plt.subplots(figsize = (15, 15))\n",
        "\n",
        "plt.subplot(3,2,1)\n",
        "labels = 'Non Risk',\"Risk\"\n",
        "plt.pie(df['ten_year_chd'].value_counts(), labels=labels ,autopct='%1.0f%%')\n",
        "plt.title(\"Cardiovascular Risk rate\")\n",
        "\n",
        "plt.subplot(3,2,2)\n",
        "labels = 'Not Under BP medication',\"under BP medication\"\n",
        "plt.pie(df['bp_meds'].value_counts(), labels=labels ,autopct='%1.0f%%')\n",
        "plt.title(\"Blood pressure rate of people\")\n",
        "\n",
        "plt.subplot(3,2,3)\n",
        "labels = 'No Stroke','Stroke'\n",
        "plt.pie(df['prevalent_stroke'].value_counts(), labels=labels ,autopct='%1.0f%%')\n",
        "plt.title(\"% people who had Stroke previously\")\n",
        "\n",
        "plt.subplot(3,2,4)\n",
        "labels = 'Not hyper tensive','hypertensive'\n",
        "plt.pie(df['prevalent_hyp'].value_counts(), labels=labels ,autopct='%1.0f%%')\n",
        "plt.title(\"% people who had hypertension previously\")\n",
        "\n",
        "plt.subplot(3,2,5)\n",
        "labels = 'No diabetis','having diabetis'\n",
        "plt.pie(df['diabetes'].value_counts(), labels=labels ,autopct='%1.0f%%')\n",
        "plt.title(\"% people who had diabetes \")\n",
        "\n",
        "plt.subplot(3,2,6)\n",
        "labels = 'education 1','education 2','education 3','education 4'\n",
        "plt.pie(df['education'].value_counts(), labels=labels ,autopct='%1.0f%%')\n",
        "plt.title(\"Education level of people \")\n",
        "\n",
        "plt.subplots_adjust(hspace= 0.1, wspace= 0.1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " a pie chart is a circular statistical graphic that is divided into slices to illustrate numerical proportions. Each slice represents a proportion of the whole, and the size of each slice is proportional to the quantity it represents. Pie charts are commonly used to show the composition of a categorical variable or to visualize the distribution of data. They are particularly useful for presenting data with a small number of categories and for highlighting the relative sizes of different categories within a dataset."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) There are 85% of people are actually not at risk of Cardio Vascular Risk.\n",
        "\n",
        "(2) There are only 3% of people who are under BP medication.\n",
        "\n",
        "(3) There are only 1% of people who had stroke previously.\n",
        "\n",
        "(4) There are 32% of people who are having Hyper Tension.\n",
        "\n",
        "(5) There are 97% of the people who are non diabetic.\n",
        "\n",
        "(6) There are 11%(least) of the people are having highest level education and 42%(highest) of the people are having basic education level."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals the percentage composition of different categories. These insights can guide businesses in developing targeted strategies and interventions to address specific demographic groups or risk factors. For example, businesses can design preventive measures, educational campaigns, or tailored treatments to reduce the occurrence of CHD in high-risk categories. By leveraging these insights, businesses in the healthcare industry can improve patient outcomes, enhance customer satisfaction, and drive positive growth.\n",
        "\n",
        "There are no specific insights from the chart that directly lead to negative growth. However, it's important to consider the overall prevalence of CHD and the effectiveness of interventions. If the prevalence of CHD remains high across all categories and the implemented strategies fail to yield desired outcomes, it could potentially result in negative growth due to increased healthcare costs, decreased patient satisfaction, or reputational issues. Therefore, the business impact ultimately depends on the successful implementation of strategies based on the insights gained from the chart."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df.copy()"
      ],
      "metadata": {
        "id": "tg95XnwkzeOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "def heartRate_data(row):\n",
        "    if row['heart_rate'] <= 59:\n",
        "        value = 'Low'\n",
        "    elif row['heart_rate'] < 100:\n",
        "        value = 'Normal'\n",
        "    else:\n",
        "        value = \"High\"\n",
        "\n",
        "    return value\n",
        "\n",
        "df1['heartRateLabel'] = df.apply(heartRate_data, axis = 1)\n",
        "\n",
        "df1['heartRateLabel'].value_counts"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,5))\n",
        "my_palette = {0 : 'orange' , 1 : 'teal'}\n",
        "sns.countplot(x = df1['heartRateLabel'], hue = df1['ten_year_chd'], palette = my_palette)\n",
        "plt.title(\"Is Heart rate is responsible for CHD ?? \")\n",
        "plt.legend(['No Risk','At Risk'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lY6N39BfetY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "bar chart is a good visualization tool for categorical variable to count different values in our feature"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above graph clearly shows that The normal heart rate is at high No risk zone while remainig two we cant clearly tell."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart count the different values of given featere. These insights can guide businesses in developing targeted strategies and interventions to address specific demographic groups or risk factors. For example, businesses can design preventive measures, educational campaigns, or tailored treatments to reduce the occurrence of CHD in high-risk categories. By leveraging these insights, businesses in the healthcare industry can improve patient outcomes, enhance customer satisfaction, and drive positive growth.\n",
        "\n",
        "There are no specific insights from the chart that directly lead to negative growth. However, it's important to consider the overall prevalence of CHD and the effectiveness of interventions. If the prevalence of CHD remains high across all categories and the implemented strategies fail to yield desired outcomes, it could potentially result in negative growth due to increased healthcare costs, decreased patient satisfaction, or reputational issues. Therefore, the business impact ultimately depends on the successful implementation of strategies based on the insights gained from the chart."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "#Min max scaler\n",
        "column_names = continuous_var\n",
        "# column_names\n",
        "#taking columns to do the minmaxscaling\n",
        "cardio_2 = pd.DataFrame()\n",
        "#using standardization as both numeric columns are in different scale\n",
        "scaler = MinMaxScaler()\n",
        "scaled = scaler.fit_transform(df[continuous_var])\n",
        "#print(scaled)\n",
        "cardio_2 = pd.DataFrame(scaler.fit_transform(df[continuous_var]))\n",
        "cardio_2.columns = column_names\n",
        "\n",
        "sns.set(rc = {'figure.figsize':(15,8)})\n",
        "cardio_2.plot.kde()\n",
        "plt.title(\"Distribution plot\")\n",
        "plt.xlabel(\"Numerical variables\")"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A KDE (Kernel Density Estimate) plot is a type of data visualization that provides a smoothed estimate of the probability density function of a continuous random variable. It is used to visualize the distribution of a single continuous variable or the relationship between two continuous variables."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe that there are many people with high level of glucose followed by cig_per_day and total cholestrol. It might be that the lifestyle of people are contributing more to these values. So we observe some peaks in these vlaues."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "it show us the distribution of all the variables . These insights can guide businesses in developing targeted strategies and interventions to address specific demographic groups or risk factors. For example, businesses can design preventive measures, educational campaigns, or tailored treatments to reduce the occurrence of CHD in high-risk categories. By leveraging these insights, businesses in the healthcare industry can improve patient outcomes, enhance customer satisfaction, and drive positive growth.\n",
        "\n",
        "There are no specific insights from the chart that directly lead to negative growth. However, it's important to consider the overall prevalence of CHD and the effectiveness of interventions. If the prevalence of CHD remains high across all categories and the implemented strategies fail to yield desired outcomes, it could potentially result in negative growth due to increased healthcare costs, decreased patient satisfaction, or reputational issues. Therefore, the business impact ultimately depends on the successful implementation of strategies based on the insights gained from the chart."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "# Drop non-numeric columns\n",
        "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr = numeric_df.corr()\n",
        "\n",
        "mask = np.zeros_like(corr)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "with sns.axes_style(\"white\"):\n",
        "    f, ax = plt.subplots(figsize=(18, 9))\n",
        "    ax = sns.heatmap(corr, mask=mask, vmin=-1, vmax=1, annot=True, cmap=\"YlGnBu\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation coefficient is a measure of the strength and direction of a linear relationship between two variables. A correlation matrix is used to summarize the relationships among a set of variables and is an important tool for data exploration and for selecting which variables to include in a model. The range of correlation is [-1,1].\n",
        "\n",
        "Thus to know the correlation between all the variables along with the correlation coeficients, we have used correlation heatmap."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see that the systolic_bp and diastolic_bp are highly correlated, also diabetes and glucose have a correlation of 0.62. Education is not important as the CHD will not be acquired based on education level of a person so we can drop education column."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df, hue=\"ten_year_chd\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pairplot, also known as a scatterplot matrix, is a visualization that allows you to visualize the relationships between all pairs of variables in a dataset. It is a useful tool for data exploration because it allows you to quickly see how all of the variables in a dataset are related to one another.\n",
        "\n",
        "Thus, we used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the cigs_per_day data distribution is highly skewed and it contains high 0 value so we can convert this into categorical column."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null hypothesis: There is no association between education level and CHD outcome.\n",
        "\n",
        "Alternate hypothesis: There is an association between education level and CHD outcome."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Create contingency table\n",
        "contingency_table = pd.crosstab(df['education'], df['ten_year_chd'])\n",
        "print(contingency_table)\n",
        "\n",
        "# Perform chi-squared test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "# Print p-value\n",
        "print(f'p-value: {p}')"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test the hypothesis that the ‘education’ column does not impact the outcome of chronic heart disease (CHD), I performed a chi-squared test of independence. This statistical test allowed me to determine if there was a significant association between education level and CHD outcome. By calculating the chi-squared statistic and p-value, I was able to make a statistical inference about the relationship between these two variables in our dataset.\n"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose to use the chi-squared test of independence to test the hypothesis that the ‘education’ column does not impact the outcome of chronic heart disease (CHD) because it is an appropriate statistical test for determining if there is a significant association between two categorical variables. In this case, both education level and CHD outcome are categorical variables, so the chi-squared test is a suitable choice.\n",
        "\n",
        "The chi-squared test works by comparing the observed frequency distribution of the data in a contingency table to the expected frequency distribution under the assumption that the null hypothesis is true. If there is a significant difference between the observed and expected frequencies, it suggests that there is an association between the two variables.\n",
        "\n",
        "Overall, I chose to use the chi-squared test of independence because it is a widely used and well-established statistical test for analyzing the relationship between two categorical variables. It allowed me to make a statistical inference about the relationship between education level and CHD outcome in our dataset."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "#Education Bp-meds (mode)\n",
        "\n",
        "# Replacing the missing values in the categorical columns with its mode\n",
        "df['education'] = df['education'].fillna(df['education'].mode()[0])\n",
        "df['bp_meds'] = df['bp_meds'].fillna(df['bp_meds'].mode()[0])\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean and median number of cigarettes per day\n",
        "df.cigs_per_day.mean().round(0),df.cigs_per_day.median()"
      ],
      "metadata": {
        "id": "qIwcDRvQHMho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All missing values in the cigs_per_day column\n",
        "df[df['cigs_per_day'].isna()]"
      ],
      "metadata": {
        "id": "AcuuZt7WHb2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean and median number of cigarettes per day for a smoker (excluding non-smokers)\n",
        "df[df['is_smoking']=='YES']['cigs_per_day'].mean(),df[df['is_smoking']=='YES']['cigs_per_day'].median()\n",
        "\n"
      ],
      "metadata": {
        "id": "1gRmr5sKHtHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputing the missing values in the cigs_per_day\n",
        "df['cigs_per_day'] = df['cigs_per_day'].fillna(df[df['is_smoking']=='YES']['cigs_per_day'].median())\n"
      ],
      "metadata": {
        "id": "dBU62ofoICbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for any wrong entries where the patient is not a smoker\n",
        "# and cigarettes per day above 0\n",
        "\n",
        "df[(df['is_smoking']=='NO') & (df['cigs_per_day']>0)]\n"
      ],
      "metadata": {
        "id": "HXsONoWnInS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for any wrong entries where the patient is a smoker\n",
        "# and cigarettes per day is 0\n",
        "\n",
        "df[(df['is_smoking']=='YES') & (df['cigs_per_day']==0)]\n"
      ],
      "metadata": {
        "id": "pAn1RiHgI8eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is observed that there are no incorrect entry."
      ],
      "metadata": {
        "id": "l-WrfPwYJIuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cholesterol - BMI - Heartrate**"
      ],
      "metadata": {
        "id": "-Y_RfhGsJO1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_2 = df"
      ],
      "metadata": {
        "id": "2BbhHHDFJfJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean and median for total_cholesterol\n",
        "data_2.total_cholesterol.mean(),data_2.total_cholesterol.median()"
      ],
      "metadata": {
        "id": "-tAaPUhfJFvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean and median for bmi\n",
        "data_2.bmi.mean(),data_2.bmi.median()"
      ],
      "metadata": {
        "id": "yHpRGjOYJuT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean and median for heart_rate\n",
        "data_2.heart_rate.mean(),data_2.heart_rate.median()"
      ],
      "metadata": {
        "id": "hbJIVsQfJvRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imputing missing values in the total_cholesterol, bmi, and heart_rate with their medain values\n",
        "data_2['total_cholesterol'] = data_2['total_cholesterol'].fillna(data_2['total_cholesterol'].median())\n",
        "data_2['bmi'] = data_2['bmi'].fillna(data_2['bmi'].median())\n",
        "data_2['heart_rate'] = data_2['heart_rate'].fillna(data_2['heart_rate'].median())"
      ],
      "metadata": {
        "id": "fsB1iwbLJxdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean and median of total_cholesterol after median imputation\n",
        "data_2.total_cholesterol.mean(),data_2.total_cholesterol.median()\n"
      ],
      "metadata": {
        "id": "6xSEg0aTJ9-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean and median of bmi after median imputation\n",
        "data_2.bmi.mean(),data_2.bmi.median()"
      ],
      "metadata": {
        "id": "aJdUr17YJ_Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean and median of heart_rate after median imputation\n",
        "data_2.heart_rate.mean(),data_2.heart_rate.median()\n"
      ],
      "metadata": {
        "id": "Ptpsl9XsKCbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Glucose**"
      ],
      "metadata": {
        "id": "Y8iWnHFlKhcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# total missing values in glucose\n",
        "data_2.glucose.isna().sum()"
      ],
      "metadata": {
        "id": "XgL8vYZfKgZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean, median, and mode for glucose\n",
        "data_2.glucose.mean(),data_2.glucose.median(),data_2.glucose.mode()\n"
      ],
      "metadata": {
        "id": "RED3aEugKoG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The spread of the data is skewed to the positive side, with values that fall outside the typical range.\n",
        "\n",
        "In the glucose column, 304 data points are missing. Using the mean or median to fill in these values could introduce significant inaccuracies.\n",
        "\n",
        "To counteract this, the KNN imputer method could be employed to replace the missing data.\n",
        "\n",
        "In a time series dataset, the missing values could be estimated through interpolation, which is a method of estimating missing data points."
      ],
      "metadata": {
        "id": "U4QawidhLBCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing the string values of the binary column with 0 and 1\n",
        "\n",
        "data_2['sex'] = np.where(data_2['sex'] == 'M',1,0)\n",
        "data_2['is_smoking'] = np.where(data_2['is_smoking'] == 'YES',1,0)"
      ],
      "metadata": {
        "id": "4CnwbImmL9oX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# changing datatypes\n",
        "data_2 = data_2.astype({'age': int,'sex':int,'is_smoking':int,'cigs_per_day':int,\n",
        "               'bp_meds':int,'prevalent_stroke':int,'prevalent_hyp':int,'diabetes':int,\n",
        "               'total_cholesterol':float,'systolic_bp':float,'diastolic_bp':float,\n",
        "               'bmi':float,'heart_rate':float,'glucose':float,'ten_year_chd':int})"
      ],
      "metadata": {
        "id": "7hj_5yA6L_GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_2.head()"
      ],
      "metadata": {
        "id": "LYW4_qJsypdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using KNN imputer with K=10\n",
        "imputer = KNNImputer(n_neighbors=10)\n",
        "imputed = imputer.fit_transform(data_2)\n",
        "data_2 = pd.DataFrame(imputed, columns=data_2.columns)\n"
      ],
      "metadata": {
        "id": "fn4javCFMCot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean, median, and mode for glucose after knn imputation\n",
        "data_2.glucose.mean(),data_2.glucose.median(),data_2.glucose.mode()\n"
      ],
      "metadata": {
        "id": "R5fM4DwuMHR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_2.info()\n",
        "\n"
      ],
      "metadata": {
        "id": "cVp4j0Y4MO-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for missing values\n",
        "data_2.isna().sum()"
      ],
      "metadata": {
        "id": "QNT_DPUiMUuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used median, mode and KNN imputation techniques to handle missing values in the dataset. I chose to use these techniques because they are appropriate for the nature of the data and the specific problem at hand. For example, I used median imputation for numerical variables that were skewed, as it is a robust measure of central tendency that is not affected by outliers.\n",
        "\n",
        "I used mode imputation for categorical variables, as it is the most common value and can be a good estimate for missing values.\n",
        "\n",
        "I also used KNN imputation, which works by finding similar observations in the dataset and using their values to impute missing values. This technique can be useful when there are patterns or relationships in the data that can be leveraged to make more accurate imputations."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "continuous_var"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing outliers\n",
        "for col in continuous_var:\n",
        "  # Using IQR method to define the range of inliners:\n",
        "  q1, q3, median = data_2[col].quantile([0.25,0.75,0.5])\n",
        "  lower_limit = q1 - 1.5*(q3-q1)\n",
        "  upper_limit = q3 + 1.5*(q3-q1)\n",
        "\n",
        "  # Replacing Outliers\n",
        "  data_2[col] = np.where(data_2[col] > upper_limit, upper_limit,np.where(\n",
        "                         data_2[col] < lower_limit,lower_limit,data_2[col]))\n"
      ],
      "metadata": {
        "id": "KlZKz61ONBHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used the Interquartile Range (IQR) method to identify and remove outliers in the continuous columns of the dataset. I chose to use this technique because it is a robust method for detecting outliers that is not affected by the presence of extreme values. The IQR is calculated as the difference between the 75th and 25th percentiles of the data, and any value that falls below the 25th percentile minus 1.5 times the IQR or above the 75th percentile plus 1.5 times the IQR is considered an outlier. By using this method, I was able to identify and remove outliers in a consistent and objective manner.\n",
        "\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "data_2 = pd.get_dummies(data_2, columns=['education'])"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_2"
      ],
      "metadata": {
        "id": "xbVpv1eEWKyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Onehot encoding is used to encode the education column.\n",
        "\n",
        "All the remaining categorical columns are binary (0/1) so no need to encode them.\n",
        "\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "#remove multicollinearity by using VIF technique\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "continuous_feature_df = pd.DataFrame(data_2[continuous_var])"
      ],
      "metadata": {
        "id": "EQ-XHxYUXK7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "continuous_feature_df"
      ],
      "metadata": {
        "id": "f9bJm6m6XNwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(data_2[[i for i in continuous_feature_df]])\n"
      ],
      "metadata": {
        "id": "okvVlpwzXhJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new column pulse_pressure and dropping systolic_bp and diastolic_bp\n",
        "\n",
        "data_2['pulse_pressure'] = data_2['systolic_bp']-data_2['diastolic_bp']\n",
        "data_2.drop('systolic_bp',axis=1,inplace=True)\n",
        "data_2.drop('diastolic_bp',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "IUEzQW9GZFMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# columns\n",
        "data_2.columns"
      ],
      "metadata": {
        "id": "6zlDIHdwZHmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating the continuous_var list\n",
        "\n",
        "continuous_var.remove('systolic_bp')\n",
        "continuous_var.remove('diastolic_bp')\n",
        "continuous_var.append('pulse_pressure')\n"
      ],
      "metadata": {
        "id": "VWNnarc2ZRfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_feature_df = pd.DataFrame(data_2[continuous_var])\n"
      ],
      "metadata": {
        "id": "hGD7ndC9Zn93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(data_2[[i for i in continuous_feature_df]])\n"
      ],
      "metadata": {
        "id": "yDkt7dyhZtku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr = data_2[continuous_var].corr()\n",
        "mask = np.zeros_like(corr)\n",
        "\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "with sns.axes_style(\"white\"):\n",
        "    f, ax = plt.subplots(figsize=(18, 9))\n",
        "    ax = sns.heatmap(corr , mask=mask, vmin = -1,vmax=1, annot = True, cmap=\"YlGnBu\")\n"
      ],
      "metadata": {
        "id": "r9WF4rxzZ7CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "# dropping is_smoking\n",
        "data_2.drop('is_smoking',axis=1,inplace=True)\n",
        "categorical_var.remove('is_smoking')\n",
        "categorical_var\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_2.columns"
      ],
      "metadata": {
        "id": "bz7boQ7xabDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used variance inflation factor to remove multicollinearity and we found that the systolic and diastolic blood pressure have high VIF, so we created a new feature which is pulse pressure.\n",
        "\n",
        "It was also found that the is smoking column just had the values yes or no for smoking and the same was conveyed in the cigs per day column where for non smoker, the column had 0 and for smoker it had the number of cigeratte per day.\n",
        "\n"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally the important colums are 'age', 'sex', 'cigs_per_day', 'bp_meds', 'prevalent_stroke', 'prevalent_hyp', 'diabetes', 'total_cholesterol', 'bmi', 'heart_rate', 'glucose', 'ten_year_chd', 'education_1.0', 'education_2.0', 'education_3.0', 'education_4.0', 'pulse_pressure'.\n",
        "\n",
        "All these columns contains the demographic, behavioural, current medical and historic medical data.\n",
        "\n"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# skewness along the index axis\n",
        "(data_2[continuous_var]).skew(axis = 0)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skew for sqrt transformation\n",
        "np.sqrt(data_2[continuous_var]).skew(axis = 0)\n"
      ],
      "metadata": {
        "id": "pjWqTcbVbfKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Skew for log10 transformation\n",
        "np.log10(data_2[continuous_var]+1).skew(axis = 0)\n"
      ],
      "metadata": {
        "id": "WDnX4LAvboSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing log transformation on continuous variables\n",
        "\n",
        "data_2['age']                   = np.log10(data_2['age']+1)\n",
        "data_2['cigs_per_day']          = np.sqrt(data_2['cigs_per_day'])\n",
        "data_2['total_cholesterol']     = np.log10(data_2['total_cholesterol']+1)\n",
        "data_2['bmi']                   = np.sqrt(data_2['bmi']+1)\n",
        "data_2['heart_rate']            = np.log10(data_2['heart_rate']+1)\n",
        "data_2['glucose']               = np.sqrt(data_2['glucose'])\n",
        "data_2['pulse_pressure']        = np.sqrt(data_2['pulse_pressure'])\n"
      ],
      "metadata": {
        "id": "1C-QyzGHcFvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking skew after log transformation\n",
        "data_2[continuous_var].skew(axis = 0)"
      ],
      "metadata": {
        "id": "Yev3H115cq4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the data need to be transformed as it was skewed.\n",
        "\n",
        "We used log transform and squareroot transform on the different continuous columns to reduce the skew of the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "UQcaC7j7dZse"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5aDjMcHndUrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "scaler = StandardScaler()\n",
        "features = [i for i in data_2.columns if i not in ['ten_year_chd']]\n",
        "\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "id": "pVPIMHq-ecy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_var\n"
      ],
      "metadata": {
        "id": "9l_3ZIfTefzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_2[continuous_var] = scaler.fit_transform(data_2[continuous_var])\n"
      ],
      "metadata": {
        "id": "Kkjs7n3genMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the X and y\n",
        "X = data_2.drop('ten_year_chd',axis=1)\n",
        "y = data_2['ten_year_chd']\n"
      ],
      "metadata": {
        "id": "iFNn53s0et-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this we have different independent features of different scale so we have used standard scalar method to scale our independent features into one scale."
      ],
      "metadata": {
        "id": "hhxQv8Rqe1VN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction is not needed. We have already reduced the number of features and only the important features are left.\n",
        "\n"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3697, stratify=y, shuffle=True)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the train distribution of dependent variable\n",
        "y_train.value_counts()\n"
      ],
      "metadata": {
        "id": "IyE0hHHEgwap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the model we have split the data into train and test using train_test_split method\n",
        "\n",
        "We have split 80% of our data into train and 20% into test, this ratio provides a good balance between having enough data to train a model effectively and having enough data to evaluate the model’s performance on unseen data. By using 80% of the data for training, the model has access to a large amount of information to learn from, while the remaining 20% of the data can be used to assess how well the model generalizes to new data.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the dataset is imbalanced and the number of positive cases is very low compared to the negative cases.\n",
        "\n"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "# visualize the target variable before SMOTE\n",
        "y_train.value_counts().plot(kind='bar', title='Target variable before SMOTE')\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Oversampling using SMOTETomek\n",
        "# fit predictor and target variable\n",
        "X_smote, y_smote = SMOTETomek(random_state=0).fit_resample(X_train, y_train)\n",
        "\n",
        "print('Samples in the original dataset: ', len(y_train))\n",
        "print('Samples in the resampled dataset: ', len(y_smote))\n"
      ],
      "metadata": {
        "id": "jhFXpX7ChQ3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the target variable after SMOTE\n",
        "y_smote.value_counts().plot(kind='bar', title='Target variable after SMOTE')\n"
      ],
      "metadata": {
        "id": "HO-z6JfYhlv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the SMOTE combined with Tomek links oversampling technique to handle the imbalanced dataset. SMOTE works by generating synthetic samples from the minority class to balance the class distribution. It does this by selecting instances that are close in the feature space and drawing a line between them, then creating new instances along this line. Tomek links oversampling removes Tomek links, which are pairs of instances from different classes that are very close to each other in the feature space. By removing these instances, the decision boundary between the classes can be made clearer.\n",
        "\n",
        "I chose to use this technique because it can improve the performance of machine learning models on imbalanced datasets by balancing the class distribution and making the decision boundary between classes clearer. By combining oversampling of the minority class with undersampling of the majority class, I was able to achieve a balanced dataset while also removing potential noise and ambiguity from the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    '''The function will take model, x train, x test, y train, y test\n",
        "    and then it will fit the model, then make predictions on the trained model,\n",
        "    it will then print roc-auc score of train and test, then plot the roc, auc curve,\n",
        "    print confusion matrix for train and test, then print classification report for train and test,\n",
        "    then plot the feature importances if the model has feature importances,\n",
        "    and finally it will return the following scores as a list:\n",
        "    recall_train, recall_test, acc_train, acc_test, roc_auc_train, roc_auc_test, F1_train, F1_test\n",
        "    '''\n",
        "\n",
        "    # fit the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # make predictions on the test data\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    pred_prob_train = model.predict_proba(X_train)[:,1]\n",
        "    pred_prob_test = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "    # calculate ROC AUC score\n",
        "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
        "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
        "    print(\"\\nTrain ROC AUC:\", roc_auc_train)\n",
        "    print(\"Test ROC AUC:\", roc_auc_test)\n",
        "\n",
        "    # plot the ROC curve\n",
        "    fpr_train, tpr_train, thresholds_train = roc_curve(y_train, pred_prob_train)\n",
        "    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, pred_prob_test)\n",
        "    plt.plot([0,1],[0,1],'k--')\n",
        "    plt.plot(fpr_train, tpr_train, label=\"Train ROC AUC: {:.2f}\".format(roc_auc_train))\n",
        "    plt.plot(fpr_test, tpr_test, label=\"Test ROC AUC: {:.2f}\".format(roc_auc_test))\n",
        "    plt.legend()\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.show()\n",
        "\n",
        "    # calculate confusion matrix\n",
        "    cm_train = confusion_matrix(y_train, y_pred_train)\n",
        "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    sns.heatmap(cm_train, annot=True, xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], cmap=\"BuPu\", fmt='.4g', ax=ax[0])\n",
        "    ax[0].set_xlabel(\"Predicted Label\")\n",
        "    ax[0].set_ylabel(\"True Label\")\n",
        "    ax[0].set_title(\"Train Confusion Matrix\")\n",
        "\n",
        "    sns.heatmap(cm_test, annot=True, xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], cmap=\"BuPu\", fmt='.4g', ax=ax[1])\n",
        "    ax[1].set_xlabel(\"Predicted Label\")\n",
        "    ax[1].set_ylabel(\"True Label\")\n",
        "    ax[1].set_title(\"Test Confusion Matrix\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # calculate classification report\n",
        "    cr_train = classification_report(y_train, y_pred_train, output_dict=True)\n",
        "    cr_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
        "    print(\"\\nTrain Classification Report:\")\n",
        "    crt = pd.DataFrame(cr_train).T\n",
        "    print(crt.to_markdown())\n",
        "    # sns.heatmap(pd.DataFrame(cr_train).T.iloc[:, :-1], annot=True, cmap=\"Blues\")\n",
        "    print(\"\\nTest Classification Report:\")\n",
        "    crt2 = pd.DataFrame(cr_test).T\n",
        "    print(crt2.to_markdown())\n",
        "    # sns.heatmap(pd.DataFrame(cr_test).T.iloc[:, :-1], annot=True, cmap=\"Blues\")\n",
        "\n",
        "    try:\n",
        "      try:\n",
        "        feature_importance = model.feature_importances_\n",
        "      except:\n",
        "        feature_importance = model.coef_\n",
        "      feature_importance = np.absolute(feature_importance)\n",
        "      if len(feature_importance)==len(features):\n",
        "        pass\n",
        "      else:\n",
        "        feature_importance = feature_importance[0]\n",
        "\n",
        "\n",
        "      feat = pd.Series(feature_importance, index=features)\n",
        "      feat = feat.sort_values(ascending=True)\n",
        "      plt.figure(figsize=(10,6))\n",
        "      plt.title('Feature Importances for '+str(model), fontsize = 18)\n",
        "      plt.xlabel('Relative Importance')\n",
        "      feat.plot(kind='barh')\n",
        "    except AttributeError:\n",
        "        print(\"\\nThe model does not have feature importances attribute.\")\n",
        "\n",
        "    precision_train = cr_train['weighted avg']['precision']\n",
        "    precision_test = cr_test['weighted avg']['precision']\n",
        "\n",
        "    recall_train = cr_train['weighted avg']['recall']\n",
        "    recall_test = cr_test['weighted avg']['recall']\n",
        "\n",
        "    acc_train = accuracy_score(y_true = y_train, y_pred = y_pred_train)\n",
        "    acc_test = accuracy_score(y_true = y_test, y_pred = y_pred_test)\n",
        "\n",
        "    F1_train = cr_train['weighted avg']['f1-score']\n",
        "    F1_test = cr_test['weighted avg']['f1-score']\n",
        "\n",
        "    model_score = [precision_train, precision_test, recall_train, recall_test, acc_train, acc_test, roc_auc_train, roc_auc_test, F1_train, F1_test ]\n",
        "    return model_score\n",
        "\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a score dataframe\n",
        "score = pd.DataFrame(index = ['Precision Train', 'Precision Test','Recall Train','Recall Test','Accuracy Train', 'Accuracy Test','ROC-AUC Train', 'ROC-AUC Test','F1 macro Train', 'F1 macro Test'])\n"
      ],
      "metadata": {
        "id": "LVL1ZG73cDN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "lr_model = LogisticRegression(fit_intercept=True, max_iter=10000)\n",
        "\n",
        "# model is trained (fit ) and predicted in the evaluate model"
      ],
      "metadata": {
        "id": "92MUXAevcVH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "lr_score = evaluate_model(lr_model, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['Logistic regression'] = lr_score\n",
        "score\n"
      ],
      "metadata": {
        "id": "PaIbbGVFgDdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "param_grid = {'C': [100,10,1,0.1,0.01,0.001,0.0001],\n",
        "              'penalty': ['l1', 'l2'],\n",
        "              'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
        "\n",
        "# Initializing the logistic regression model\n",
        "logreg = LogisticRegression(fit_intercept=True, max_iter=10000, random_state=0)\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=4, random_state=0)\n",
        "\n",
        "# Using GridSearchCV to tune the hyperparameters using cross-validation\n",
        "grid = GridSearchCV(logreg, param_grid, cv=rskf)\n",
        "grid.fit(X_smote, y_smote)\n",
        "\n",
        "best_params = grid.best_params_\n",
        "# The best hyperparameters found by GridSearchCV\n",
        "print(\"Best hyperparameters: \", best_params)\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate model with best parameters\n",
        "lr_model2 = LogisticRegression(C=best_params['C'],\n",
        "                                  penalty=best_params['penalty'],\n",
        "                                  solver=best_params['solver'],\n",
        "                                  max_iter=10000, random_state=0)"
      ],
      "metadata": {
        "id": "HkNoPb2Ti_j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "lr_score2 = evaluate_model(lr_model2, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "9uP_AEl2jOYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['Logistic regression tuned'] = lr_score2"
      ],
      "metadata": {
        "id": "8rL9wu8Wjfk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used is GridSearchCV. GridSearchCV is a method that performs an exhaustive search over a specified parameter grid to find the best hyperparameters for a model. It is a popular method for hyperparameter tuning because it is simple to implement and can be effective in finding good hyperparameters for a model.\n",
        "\n",
        "The choice of hyperparameter optimization technique depends on various factors such as the size of the parameter space, the computational resources available, and the time constraints. GridSearchCV can be a good choice when the parameter space is relatively small and computational resources are not a major concern.\n",
        "\n"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score"
      ],
      "metadata": {
        "id": "kXJy-ZBxLm1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that hyperparameter tuning did not improve the performance of the Logistic Regression model on the test set. The precision, recall, accuracy, ROC-AUC, and F1 scores on the test set are the same for both the untuned and tuned Logistic Regression models."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "dt = DecisionTreeClassifier(random_state=20)\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "dt_score = evaluate_model(dt, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "iv3AIJyaMOxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['Decision Tree'] = dt_score\n",
        "score\n"
      ],
      "metadata": {
        "id": "2c3nsVHKMd-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid\n",
        "grid = {'max_depth' : [3,4,5,6,7,8],\n",
        "        'min_samples_split' : np.arange(2,8),\n",
        "        'min_samples_leaf' : np.arange(10,20)}\n",
        "\n",
        "# Initialize the model\n",
        "model = DecisionTreeClassifier()\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=0)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(model, grid, cv=rskf)\n",
        "\n",
        "# Fit the GridSearchCV to the training data\n",
        "grid_search.fit(X_smote, y_smote)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "best_params"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a new model with the best hyperparameters\n",
        "dt2 = DecisionTreeClassifier(max_depth=best_params['max_depth'],\n",
        "                                 min_samples_leaf=best_params['min_samples_leaf'],\n",
        "                                 min_samples_split=best_params['min_samples_split'],\n",
        "                                 random_state=20)\n"
      ],
      "metadata": {
        "id": "x0TtKjNNMzbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt2_score = evaluate_model(dt2, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "xfQAVXWPM3G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['Decision Tree tuned'] = dt2_score\n"
      ],
      "metadata": {
        "id": "wApns-cJM8Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used is GridSearchCV. GridSearchCV is a method that performs an exhaustive search over a specified parameter grid to find the best hyperparameters for a model. It is a popular method for hyperparameter tuning because it is simple to implement and can be effective in finding good hyperparameters for a model.\n",
        "\n",
        "The choice of hyperparameter optimization technique depends on various factors such as the size of the parameter space, the computational resources available, and the time constraints. GridSearchCV can be a good choice when the parameter space is relatively small and computational resources are not a major concern.\n",
        "\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score"
      ],
      "metadata": {
        "id": "HWqW_OmGNFaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that hyperparameter tuning improved the performance of the Decision Tree model on the test set. The tuned Decision Tree model has higher precision and F1 score on the test set compared to the untuned Decision Tree model. However, the recall, accuracy, and ROC-AUC scores on the test set decreased slightly after tuning.\n",
        "\n",
        "The tuned model is not overfitting like the untuned model."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RANDOM FOREST**"
      ],
      "metadata": {
        "id": "ZnAN6w45PvVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "rf = RandomForestClassifier(random_state=0)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "rf_score = evaluate_model(rf, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['Random Forest'] = rf_score\n",
        "score"
      ],
      "metadata": {
        "id": "iPsY4-A-QJZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid\n",
        "grid = {'n_estimators': [10, 50, 100, 200],\n",
        "              'max_depth': [8, 9, 10, 11, 12,13, 14, 15],\n",
        "              'min_samples_split': [2, 3, 4, 5]}\n",
        "\n",
        "# Initialize the model\n",
        "rf = RandomForestClassifier(random_state=0)\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=0)\n",
        "\n",
        "# Initialize RandomSearchCV\n",
        "random_search = RandomizedSearchCV(rf, grid,cv=rskf, n_iter=10, n_jobs=-1)\n",
        "\n",
        "# Fit the GridSearchCV to the training data\n",
        "random_search.fit(X_smote, y_smote)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "best_params"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model with best parameters\n",
        "rf2 = RandomForestClassifier(n_estimators = best_params['n_estimators'],\n",
        "                                 min_samples_leaf= best_params['min_samples_split'],\n",
        "                                 max_depth = best_params['max_depth'],\n",
        "                                 random_state=0)\n"
      ],
      "metadata": {
        "id": "ZOMc28joQcwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "rf2_score = evaluate_model(rf2, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "u-RmaUmTQfFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['Random Forest tuned'] = rf2_score\n"
      ],
      "metadata": {
        "id": "300lUrQxQiC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used is RandomizedSearchCV. RandomizedSearchCV is a method that performs a random search over a specified parameter grid to find the best hyperparameters for a model. It is a popular method for hyperparameter tuning because it can be more efficient than exhaustive search methods like GridSearchCV when the parameter space is large.\n",
        "\n",
        "The choice of hyperparameter optimization technique depends on various factors such as the size of the parameter space, the computational resources available, and the time constraints. RandomizedSearchCV can be a good choice when the parameter space is large and computational resources are limited.\n",
        "\n"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score"
      ],
      "metadata": {
        "id": "Kr_-MjJSQyIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that hyperparameter tuning improved the performance of the Random Forest model on the test set. The tuned Random Forest model has higher precision, recall, accuracy, and F1 score on the test set compared to the untuned Random Forest model. The ROC-AUC score on the test set also improved slightly after tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ML Model - 4 : SVM (Support Vector Machine)**"
      ],
      "metadata": {
        "id": "PCIvcwI-RWLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the model\n",
        "svm = SVC(kernel='linear', random_state=0, probability=True)"
      ],
      "metadata": {
        "id": "1-FjQm_xRVME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "mnCum1b5RngD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "svm_score = evaluate_model(svm, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "FpOYXT1GRg1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['SVM'] = svm_score\n",
        "score\n"
      ],
      "metadata": {
        "id": "sbz8AnDnRkRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "1ZS3DigIRx-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': np.arange(0.1, 10, 0.1),\n",
        "              'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "              'degree': np.arange(2, 6, 1)}\n",
        "\n",
        "# Initialize the model\n",
        "svm2 = SVC(random_state=0, probability=True)\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=0)\n",
        "\n",
        "# Initialize RandomizedSearchCV with 6-fold cross-validation\n",
        "random_search = RandomizedSearchCV(svm2, param_grid, n_iter=10, cv=rskf, n_jobs=-1)\n",
        "\n",
        "# Fit the RandomizedSearchCV to the training data\n",
        "random_search.fit(X_smote, y_smote)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "best_params\n"
      ],
      "metadata": {
        "id": "q4GJx2FSRkgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model with best parameters\n",
        "svm2 = SVC(C = best_params['C'],\n",
        "           kernel = best_params['kernel'],\n",
        "           degree = best_params['degree'],\n",
        "           random_state=0, probability=True)\n"
      ],
      "metadata": {
        "id": "HMjPqSKyRktZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "svm2_score = evaluate_model(svm2, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "ILse7m_qRk6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['SVM tuned'] = svm2_score"
      ],
      "metadata": {
        "id": "nXe-RNh-RlmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "3UkSPux4R9IJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here Randomized search is used. Randomized search is a popular technique because it can be more efficient than exhaustive search methods like grid search. Instead of trying all possible combinations of hyperparameters, randomized search samples a random subset of the hyperparameter space. This can save time and computational resources while still finding good hyperparameters for the model."
      ],
      "metadata": {
        "id": "IXZAfyQYSDpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "frk0paI1SHX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score"
      ],
      "metadata": {
        "id": "etCa9FMjRl0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that hyperparameter tuning improved the performance of the SVM model on the test set. The tuned SVM model has higher recall, accuracy, and F1 score on the test set compared to the untuned SVM model. However, the precision and ROC-AUC scores on the test set decreased slightly after tuning."
      ],
      "metadata": {
        "id": "HDff_7a0SN0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ML Model - 5 : Xtreme Gradient Boosting**"
      ],
      "metadata": {
        "id": "H2CV7iv7SRdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "xgb_model = xgb.XGBClassifier()\n"
      ],
      "metadata": {
        "id": "y1qTBdiHRmAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n"
      ],
      "metadata": {
        "id": "qJ5wYG0eSXtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "xgb_score = evaluate_model(xgb_model, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "5gaJv3haRmM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['XGB'] = xgb_score\n",
        "score\n"
      ],
      "metadata": {
        "id": "7hbOw6ZXRmZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "zz7kzUD_Sedo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid\n",
        "param_grid = {'learning_rate': np.arange(0.01, 0.3, 0.01),\n",
        "              'max_depth': np.arange(3, 15, 1),\n",
        "              'n_estimators': np.arange(100, 200, 10)}\n",
        "\n",
        "# Initialize the model\n",
        "xgb2_model = xgb.XGBClassifier(random_state=0)\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=0)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(xgb2_model, param_grid, n_iter=10, cv=rskf)\n",
        "\n",
        "# Fit the RandomizedSearchCV to the training data\n",
        "random_search.fit(X_smote, y_smote)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "best_params"
      ],
      "metadata": {
        "id": "TP9QfXriSgKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model with best parameters\n",
        "xgb2_model = xgb.XGBClassifier(learning_rate = best_params['learning_rate'],\n",
        "                                 max_depth = best_params['max_depth'],\n",
        "                               n_estimators = best_params['n_estimators'],\n",
        "                                 random_state=0)\n"
      ],
      "metadata": {
        "id": "k7pEDZXgSlFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "xgb2_score = evaluate_model(xgb2_model, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "du1oa2UzSnU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['XGB tuned'] = xgb2_score\n"
      ],
      "metadata": {
        "id": "SxCfKvzwSqmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Qfl1j69jTJX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score"
      ],
      "metadata": {
        "id": "zNpRuucdTLcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that hyperparameter tuning improved the performance of the XGBoost model on the test set. The tuned XGBoost model has higher precision, recall, accuracy, and F1 score on the test set compared to the untuned XGBoost model. The ROC-AUC score on the test set also improved slightly after tuning."
      ],
      "metadata": {
        "id": "iqbAulGKTKg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "VG12pLIhStIB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have used Randomized search to tune the XGB model.\n",
        "\n",
        "Randomized search is a popular technique because it can be more efficient than exhaustive search methods like grid search. Instead of trying all possible combinations of hyperparameters, randomized search samples a random subset of the hyperparameter space. This can save time and computational resources while still finding good hyperparameters for the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "ploDv_TASyF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ML Model - 6 : Naive Bayes**"
      ],
      "metadata": {
        "id": "U2q04DwTTtcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate model\n",
        "naive = GaussianNB()\n"
      ],
      "metadata": {
        "id": "dM9Z3okxTsj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "S0GxdSVPT1pa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "naive_score = evaluate_model(naive, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "jCaQrlLHT4Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['Naive Bayes'] = naive_score\n",
        "score\n"
      ],
      "metadata": {
        "id": "fcsrA1hQT-O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "_GCRdJ4gUCt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid\n",
        "param_grid = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
        "# Initialize the model\n",
        "naive = GaussianNB()\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=4, n_repeats=4, random_state=0)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = GridSearchCV(naive, param_grid, cv=rskf, n_jobs=-1)\n",
        "\n",
        "# Fit the RandomizedSearchCV to the training data\n",
        "random_search.fit(X_smote, y_smote)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "best_params\n"
      ],
      "metadata": {
        "id": "RcBUAELIUBtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate model with best parameters\n",
        "naive2 = GaussianNB(var_smoothing = best_params['var_smoothing'])\n"
      ],
      "metadata": {
        "id": "2kh8ZmT-UO-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "naive2_score = evaluate_model(naive2, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "I_9v4rOIURgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['Naive Bayes tuned']=naive2_score\n"
      ],
      "metadata": {
        "id": "dnhBhdHeUU2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which hyperparameter optimization technique have you used and why?\n",
        "Here we have used the gridsearch for optimization of the Naive Bayes model.\n",
        "\n",
        "Grid search is an exhaustive search method that tries all possible combinations of hyperparameters specified in the hyperparameter grid. This technique can be useful when the number of hyperparameters to tune is small and the range of possible values for each hyperparameter is limited. Grid search can find the best combination of hyperparameters, but it can be computationally expensive for large hyperparameter grids."
      ],
      "metadata": {
        "id": "v3clpOJgUOQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "CU-tNBg2Uct3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score"
      ],
      "metadata": {
        "id": "yE31LTA7Ufu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that hyperparameter tuning improved the performance of the Naive Bayes model on the test set. The tuned Naive Bayes model has higher precision, recall, accuracy, and F1 score on the test set compared to the untuned Naive Bayes model. The ROC-AUC score on the test set also improved slightly after tuning.\n",
        "\n"
      ],
      "metadata": {
        "id": "fGry7zyAUkBi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ML Model - 7 : Neural Network**"
      ],
      "metadata": {
        "id": "JNsxzjVRUokK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate model\n",
        "neural = MLPClassifier(random_state=0)\n"
      ],
      "metadata": {
        "id": "Es-20AOyUtkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "CDCa26yyUw7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "neural_score = evaluate_model(neural, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "Rxl8kCj8UteO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['Neural Network'] = neural_score\n",
        "score\n",
        "\n"
      ],
      "metadata": {
        "id": "aR1OiiHZUtVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "r51gppraU4zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid\n",
        "param_grid = {'hidden_layer_sizes': np.arange(10, 100, 10),\n",
        "              'alpha': np.arange(0.0001, 0.01, 0.0001)}\n",
        "# Initialize the model\n",
        "neural = MLPClassifier(random_state=0)\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=0)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(neural, param_grid, n_iter=10, cv=rskf, n_jobs=-1)\n",
        "\n",
        "# Fit the RandomizedSearchCV to the training data\n",
        "random_search.fit(X_smote, y_smote)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "best_params"
      ],
      "metadata": {
        "id": "pQ-TmRD2UtEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate model with best parameters\n",
        "neural2 = MLPClassifier(hidden_layer_sizes = best_params['hidden_layer_sizes'],\n",
        "                        alpha = best_params['alpha'],\n",
        "                        random_state = 0)\n"
      ],
      "metadata": {
        "id": "fJ_7x8xmUs9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "neural2_score = evaluate_model(neural2, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "qwvkX1aSUs3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score['Neural Network tuned']=neural2_score\n"
      ],
      "metadata": {
        "id": "mvHW7sSOUsxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which hyperparameter optimization technique have you used and why?\n",
        "Here we have used Randomized search to tune the Neural Network model.\n",
        "\n",
        "Randomized search is a popular technique because it can be more efficient than exhaustive search methods like grid search. Instead of trying all possible combinations of hyperparameters, randomized search samples a random subset of the hyperparameter space. This can save time and computational resources while still finding good hyperparameters for the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "I0unNo5WVCBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "oOtirXRwVKGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score\n"
      ],
      "metadata": {
        "id": "-zo_GRJoUsmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that hyperparameter tuning improved the performance of the neural network model on the test set. The tuned neural network has higher precision, recall, accuracy, and F1 score on the test set compared to the untuned neural network. The ROC-AUC score on the test set also improved slightly after tuning."
      ],
      "metadata": {
        "id": "vI7liFhaVOc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot of scores for models**"
      ],
      "metadata": {
        "id": "nIuPBFYxVTIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Precision***"
      ],
      "metadata": {
        "id": "VxWdiVaxVc2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# precision Scores plot\n",
        "\n",
        "models = list(score.columns)\n",
        "train = score.iloc[0,:]\n",
        "test = score.iloc[1,:]\n",
        "\n",
        "X_axis = np.arange(len(models))\n",
        "\n",
        "plt.figure(figsize=(25,10))\n",
        "plt.bar(X_axis - 0.2, train, 0.4, label = 'Train Precision')\n",
        "plt.bar(X_axis + 0.2, test, 0.4, label = 'Test Precision')\n",
        "\n",
        "\n",
        "plt.xticks(X_axis,models, rotation=30)\n",
        "plt.ylabel(\"Precision Score\")\n",
        "plt.title(\"Precision score for each model\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D1KbxcZ3ViIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall"
      ],
      "metadata": {
        "id": "N10_7iZ5Vx9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall Scores plot\n",
        "\n",
        "models = list(score.columns)\n",
        "train = score.iloc[2,:]\n",
        "test = score.iloc[3,:]\n",
        "\n",
        "X_axis = np.arange(len(models))\n",
        "\n",
        "plt.figure(figsize=(25,10))\n",
        "plt.bar(X_axis - 0.2, train, 0.4, label = 'Train Recall')\n",
        "plt.bar(X_axis + 0.2, test, 0.4, label = 'Test Recall')\n",
        "\n",
        "\n",
        "plt.xticks(X_axis,models, rotation=30)\n",
        "plt.ylabel(\"Recall Score\")\n",
        "plt.title(\"Recall score for each model\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MvDiBPVvVt1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy"
      ],
      "metadata": {
        "id": "LHh_xe1WV5XX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy Scores plot\n",
        "\n",
        "models = list(score.columns)\n",
        "train = score.iloc[4,:]\n",
        "test = score.iloc[5,:]\n",
        "\n",
        "X_axis = np.arange(len(models))\n",
        "\n",
        "plt.figure(figsize=(25,10))\n",
        "plt.bar(X_axis - 0.2, train, 0.4, label = 'Train Accuracy')\n",
        "plt.bar(X_axis + 0.2, test, 0.4, label = 'Test Accuracy')\n",
        "\n",
        "\n",
        "plt.xticks(X_axis,models, rotation=30)\n",
        "plt.ylabel(\"Accuracy Score\")\n",
        "plt.title(\"Accuracy score for each model\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cTyiGG6eVvGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROC-AUC"
      ],
      "metadata": {
        "id": "P3iaoyepV9q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC-AUC Scores plot\n",
        "\n",
        "models = list(score.columns)\n",
        "train = score.iloc[6,:]\n",
        "test = score.iloc[7,:]\n",
        "\n",
        "X_axis = np.arange(len(models))\n",
        "\n",
        "plt.figure(figsize=(25,10))\n",
        "plt.bar(X_axis - 0.2, train, 0.4, label = 'Train ROC-AUC')\n",
        "plt.bar(X_axis + 0.2, test, 0.4, label = 'Test ROC-AUC')\n",
        "\n",
        "\n",
        "plt.xticks(X_axis,models, rotation=30)\n",
        "plt.ylabel(\"ROC-AUC Score\")\n",
        "plt.title(\"ROC-AUC score for each model\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kttBGTJ8VvXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1 score\n"
      ],
      "metadata": {
        "id": "dS4p9F7nWB6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# F1 macro Scores plot\n",
        "\n",
        "models = list(score.columns)\n",
        "train = score.iloc[8,:]\n",
        "test = score.iloc[9,:]\n",
        "\n",
        "X_axis = np.arange(len(models))\n",
        "\n",
        "plt.figure(figsize=(25,10))\n",
        "plt.bar(X_axis - 0.2, train, 0.4, label = 'Train F1 macro')\n",
        "plt.bar(X_axis + 0.2, test, 0.4, label = 'Test F1 macro')\n",
        "\n",
        "\n",
        "plt.xticks(X_axis,models, rotation=30)\n",
        "plt.ylabel(\"F1 macro Score\")\n",
        "plt.title(\"F1 macro score for each model\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jkGwIXb6Vvjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selection of best model"
      ],
      "metadata": {
        "id": "LGibBphrWHqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score"
      ],
      "metadata": {
        "id": "7jNqD9HeVvwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the overfitted models which have recall, rocauc, f1 for train as 1\n",
        "score_t = score.transpose()            #taking transpose of the score dataframe to create new difference column\n",
        "remove_models = score_t[score_t['Recall Train']>=0.95].index  #creating a list of models which have 1 for train and score_t['Accuracy Train']==1.0 and score_t['ROC-AUC Train']==1.0 and score_t['F1 macro Train']==1.0\n",
        "remove_models\n",
        "\n",
        "adj = score_t.drop(remove_models)                     #creating a new dataframe with required models\n",
        "adj"
      ],
      "metadata": {
        "id": "XjbgXm00Vv-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_best_model(df, metrics):\n",
        "\n",
        "    best_models = {}\n",
        "    for metric in metrics:\n",
        "        max_test = df[metric + ' Test'].max()\n",
        "        best_model_test = df[df[metric + ' Test'] == max_test].index[0]\n",
        "        best_model = best_model_test\n",
        "        best_models[metric] = best_model\n",
        "    return best_models\n"
      ],
      "metadata": {
        "id": "YZ0ZsrEWVwMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = ['Precision','Recall', 'Accuracy', 'ROC-AUC', 'F1 macro']\n",
        "\n",
        "best_models = select_best_model(adj, metrics)\n",
        "print(\"The best models are:\")\n",
        "for metric, best_model in best_models.items():\n",
        "    print(f\"{metric}: {best_model} - {adj[metric+' Test'][best_model].round(4)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "16BcFsIZWQr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After carefully considering the potential consequences of false positives and false negatives in the context of our business objectives, I have selected recall as the primary evaluation metric for our CHD risk prediction model. This means that our goal is to maximize the number of true positives (patients correctly identified as having CHD risk) while minimizing the number of false negatives (patients incorrectly identified as not having CHD risk). By doing so, we aim to ensure that we correctly identify as many patients with CHD risk as possible, even if it means that we may have some false positives."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After evaluating the performance of several machine learning models on the Framingham Heart Study dataset, I have selected the Neural Network (tuned) as our final prediction model. This decision was based on the model’s performance on our primary evaluation metric of recall, which measures the ability of the model to correctly identify patients with CHD risk. In our analysis, we found that the Neural Network (tuned) had the highest recall score among the models we evaluated.\n",
        "\n",
        "We chose recall as our primary evaluation metric because correctly identifying patients with CHD risk is critical to achieving our business objectives. By selecting a model with a high recall score, we aim to ensure that we correctly identify as many patients with CHD risk as possible, even if it means that we may have some false positives. Overall, we believe that the Neural Network (tuned) is the best choice for our needs and will help us achieve a positive business impact"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SHAP(Shapley additive Explanations)**"
      ],
      "metadata": {
        "id": "RvyzE8NPW2hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap"
      ],
      "metadata": {
        "id": "0tKRIqRIW9xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing shap\n",
        "import shap"
      ],
      "metadata": {
        "id": "ZjO01_JGXDPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X"
      ],
      "metadata": {
        "id": "YJZf1Yh-XFfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize the background dataset using k-means clustering\n",
        "X_summary = shap.kmeans(X, 100)\n",
        "\n",
        "# create an explainer object\n",
        "explainer = shap.KernelExplainer(neural2.predict_proba, X_summary)\n",
        "\n",
        "# compute the SHAP values for all the samples in the test data\n",
        "shap_values = explainer.shap_values(X_test)"
      ],
      "metadata": {
        "id": "gR4ZBt51XIQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summery plot\n",
        "shap.summary_plot(shap_values, X_test, feature_names=features)\n"
      ],
      "metadata": {
        "id": "L3tXCKKkXMzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bar plot shows the important features and the mean shap values. It shows the average impact on the model output magnitude.\n",
        "\n",
        "It does not show the positive or negative impact on the prediction."
      ],
      "metadata": {
        "id": "EzR-BLOcXBLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "# Import pickle\n",
        "import pickle\n",
        "\n",
        "# Save the best model (naive bayes tuned)\n",
        "pickle.dump(naive2, open('neural2.pkl', 'wb'))\n",
        "# Save the scaler\n",
        "pickle.dump(scaler, open('scaler.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "pickled_model = pickle.load(open('neural2.pkl', 'rb'))\n"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instance = X_test.loc[54]"
      ],
      "metadata": {
        "id": "i7DO36VwXc2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instance"
      ],
      "metadata": {
        "id": "jwL0yw7zXe_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an array for the x test value for the 50 index row\n",
        "predict_new = np.array(instance).reshape(1,-1)\n",
        "\n",
        "# Testing on one instance which we used for shap X_test[50,:]\n",
        "pickled_model.predict(predict_new)\n"
      ],
      "metadata": {
        "id": "fQeid_QtXfVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, this project demonstrated the potential of machine learning techniques to accurately predict the 10-year risk of future coronary heart disease (CHD) in patients using data from an ongoing cardiovascular study. Key points from this project include:\n",
        "\n",
        "Careful data preprocessing and transformation improved the performance of machine learning models and enabled more accurate predictions.\n",
        "Feature selection was important for identifying the most relevant predictors of CHD risk.\n",
        "The Neural Network model (tuned) was chosen as the final prediction model due to its high recall score.\n",
        "Techniques such as SMOTE combined with Tomek links undersampling and standard scalar scaling were used to handle imbalanced data and improve model performance.\n",
        "This project provides a valuable example of how machine learning techniques can be applied to real-world problems to achieve positive business impact.\n",
        "Overall, this project highlights the importance of careful data preparation and analysis in machine learning projects. By taking the time to clean and transform the data, select relevant features, and choose an appropriate model, it is possible to achieve accurate predictions and support decision-making in a wide range of domains."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c61LlqKE2t1r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}